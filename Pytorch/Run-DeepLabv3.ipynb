{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
      "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-c3_7wz1l\n",
      "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-c3_7wz1l\n",
      "Requirement already satisfied (use --upgrade to upgrade): segmentation-models-pytorch==0.1.0 from git+https://github.com/qubvel/segmentation_models.pytorch in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from segmentation-models-pytorch==0.1.0) (0.5.0)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from segmentation-models-pytorch==0.1.0) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch>=0.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from segmentation-models-pytorch==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: timm==0.1.20 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from segmentation-models-pytorch==0.1.0) (0.1.20)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (1.18.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (1.4.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (7.0.0)\n",
      "Requirement already satisfied: munch in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.0) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.0) (4.44.1)\n",
      "Building wheels for collected packages: segmentation-models-pytorch\n",
      "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.1.0-py3-none-any.whl size=53464 sha256=47b77f702e9db165bfc93f032f847cc54111ea81705ac9f4f8ddac43013d0b38\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v8f17t3k/wheels/53/e5/fc/18292d80d3c0f4efc96cbbb72625fdbafdca303997bacfb085\n",
      "Successfully built segmentation-models-pytorch\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/qubvel/segmentation_models.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchvision.transforms as standard_transforms\n",
    "import utils.joint_transforms as joint_transforms\n",
    "import utils.transforms as extended_transforms\n",
    "\n",
    "from datasets import mapillary\n",
    "from utils import CrossEntropyLoss2d\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = '/home/ec2-user/SageMaker/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'softmax2d'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "with open(os.path.join(HOME, 'config.json')) as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "CLASSES = [] # get from config\n",
    "labels = config['labels']\n",
    "for label in labels:\n",
    "    CLASSES.append(label['readable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), dilation=(4, 4), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): DeepLabV3Decoder(\n",
       "    (0): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(256, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=8.0, mode=bilinear)\n",
       "    (2): Activation(\n",
       "      (activation): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'train_batch_size': 16,\n",
    "    'epoch_num': 8,\n",
    "    'lr': 1e-10,\n",
    "    'weight_decay': 5e-4,\n",
    "    'input_size': (256, 512),\n",
    "    'momentum': 0.95,\n",
    "    'lr_patience': 100,  # large patience denotes fixed lr\n",
    "    'snapshot': '',  # empty string denotes no snapshot\n",
    "    'print_freq': 20,\n",
    "    'val_batch_size': 16,\n",
    "    'val_save_to_img_file': False,\n",
    "    'val_img_sample_rate': 0.05  # randomly sample some validation results to display\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "short_size = int(min(args['input_size']) / 0.875)\n",
    "train_joint_transform = joint_transforms.Compose([\n",
    "    joint_transforms.Scale(short_size),\n",
    "    joint_transforms.RandomCrop(args['input_size']),\n",
    "    joint_transforms.RandomHorizontallyFlip()\n",
    "])\n",
    "val_joint_transform = joint_transforms.Compose([\n",
    "    joint_transforms.Scale(short_size),\n",
    "    joint_transforms.CenterCrop(args['input_size'])\n",
    "])\n",
    "input_transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(),\n",
    "    standard_transforms.Normalize(*mean_std)\n",
    "])\n",
    "target_transform = extended_transforms.MaskToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mapillary.Mapillary('semantic', 'training', joint_transform=train_joint_transform,\n",
    "                                    transform=input_transform, target_transform=target_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=args['train_batch_size'], num_workers=12, shuffle=True, \n",
    "                            pin_memory=True)\n",
    "val_set = mapillary.Mapillary('semantic', 'validation', joint_transform=val_joint_transform, transform=input_transform,\n",
    "                                target_transform=target_transform)\n",
    "val_loader = DataLoader(val_set, batch_size=args['val_batch_size'], num_workers=4, shuffle=False,\n",
    "                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred, target, n_classes=None):\n",
    "    if n_classes is None:    \n",
    "        assert len(pred.size()) == 4\n",
    "        n_classes = pred.size()[1]\n",
    "\n",
    "    ious = []\n",
    "    pred = pred.argmax(dim=1).view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    for cls in range(n_classes):  \n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        # Cast to long to prevent overflows\n",
    "        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item() \n",
    "        union = (pred_inds.long().sum().data.cpu().item() +\n",
    "                 target_inds.long().sum().data.cpu().item() -\n",
    "                 intersection)\n",
    "\n",
    "        # do not include in evaluation if no gt\n",
    "        if union != 0:\n",
    "            ious.append(float(intersection) / float(max(union, 1)))\n",
    "\n",
    "    return torch.Tensor(ious).mean()\n",
    "\n",
    "def iou_mapillary(pred, targ, n_classes=len(CLASSES)):\n",
    "    return iou(pred, targ, n_classes)\n",
    "\n",
    "def acc_mapillary(pred, targ):\n",
    "    targ = targ.squeeze(1)\n",
    "    return (pred.argmax(dim=1)==targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = smp.utils.losses.DiceLoss()\n",
    "# metrics = [\n",
    "#     smp.utils.metrics.IoU(threshold=0.5),\n",
    "#     smp.utils.metrics.Accuracy()\n",
    "# ]\n",
    "\n",
    "class IouMapillary(smp.utils.base.Metric):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        return iou_mapillary(y_pr, y_gt)\n",
    "\n",
    "class AccuracyMapillary(smp.utils.base.Metric):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        return acc_mapillary(y_pr, y_gt)\n",
    "\n",
    "loss = CrossEntropyLoss2d(size_average=False).cuda()\n",
    "metrics = [\n",
    "    IouMapillary(),\n",
    "    AccuracyMapillary()\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CrossEntropyLoss2d'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 14 11:04:27 UTC 2020\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 1125/1125 [12:23<00:00,  1.51it/s, CrossEntropyLoss2d - 7.326e+06, iou_mapillary - 0.07809, accuracy_mapillary - 0.7512]\n",
      "valid: 100%|██████████| 125/125 [00:23<00:00,  5.43it/s, CrossEntropyLoss2d - 8.111e+06, iou_mapillary - 0.04621, accuracy_mapillary - 0.3486]\n",
      "{'CrossEntropyLoss2d': 8110994.351999998, 'iou_mapillary': 0.04621388575434685, 'accuracy_mapillary': 0.3486188316345213}\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 1125/1125 [12:24<00:00,  1.51it/s, CrossEntropyLoss2d - 7.206e+06, iou_mapillary - 0.0912, accuracy_mapillary - 0.7813] \n",
      "valid: 100%|██████████| 125/125 [00:23<00:00,  5.30it/s, CrossEntropyLoss2d - 8.104e+06, iou_mapillary - 0.04583, accuracy_mapillary - 0.3512]\n",
      "{'CrossEntropyLoss2d': 8103616.595999998, 'iou_mapillary': 0.04583303195238113, 'accuracy_mapillary': 0.3511723823547363}\n",
      "\n",
      "Epoch: 2\n",
      "train: 100%|██████████| 1125/1125 [12:24<00:00,  1.51it/s, CrossEntropyLoss2d - 7.173e+06, iou_mapillary - 0.1007, accuracy_mapillary - 0.7962] \n",
      "valid: 100%|██████████| 125/125 [00:23<00:00,  5.29it/s, CrossEntropyLoss2d - 8.181e+06, iou_mapillary - 0.05027, accuracy_mapillary - 0.316] \n",
      "{'CrossEntropyLoss2d': 8180773.412, 'iou_mapillary': 0.05026972541213036, 'accuracy_mapillary': 0.3159501571655274}\n",
      "Model saved!\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|██████████| 1125/1125 [12:24<00:00,  1.51it/s, CrossEntropyLoss2d - 7.158e+06, iou_mapillary - 0.1056, accuracy_mapillary - 0.8033]\n",
      "valid: 100%|██████████| 125/125 [00:23<00:00,  5.31it/s, CrossEntropyLoss2d - 8.106e+06, iou_mapillary - 0.06356, accuracy_mapillary - 0.3505]\n",
      "{'CrossEntropyLoss2d': 8106477.755999998, 'iou_mapillary': 0.06356430885195735, 'accuracy_mapillary': 0.3505036582946776}\n",
      "Model saved!\n",
      "\n",
      "Epoch: 4\n",
      "train: 100%|██████████| 1125/1125 [12:24<00:00,  1.51it/s, CrossEntropyLoss2d - 7.151e+06, iou_mapillary - 0.107, accuracy_mapillary - 0.806]  \n",
      "valid: 100%|██████████| 125/125 [00:23<00:00,  5.29it/s, CrossEntropyLoss2d - 8.112e+06, iou_mapillary - 0.06171, accuracy_mapillary - 0.3475]\n",
      "{'CrossEntropyLoss2d': 8111722.072000001, 'iou_mapillary': 0.06170887491106989, 'accuracy_mapillary': 0.3474639511108399}\n",
      "\n",
      "Epoch: 5\n",
      "train: 100%|██████████| 1125/1125 [12:23<00:00,  1.51it/s, CrossEntropyLoss2d - 7.147e+06, iou_mapillary - 0.1077, accuracy_mapillary - 0.808] \n",
      "valid: 100%|██████████| 125/125 [00:23<00:00,  5.30it/s, CrossEntropyLoss2d - 8.129e+06, iou_mapillary - 0.05981, accuracy_mapillary - 0.3395]\n",
      "{'CrossEntropyLoss2d': 8128696.272000001, 'iou_mapillary': 0.05981410816311835, 'accuracy_mapillary': 0.33947553253173807}\n",
      "\n",
      "Epoch: 6\n",
      "train: 100%|██████████| 1125/1125 [12:24<00:00,  1.51it/s, CrossEntropyLoss2d - 7.144e+06, iou_mapillary - 0.1081, accuracy_mapillary - 0.8095]\n",
      "valid: 100%|██████████| 125/125 [00:23<00:00,  5.30it/s, CrossEntropyLoss2d - 8.163e+06, iou_mapillary - 0.06102, accuracy_mapillary - 0.3236]\n",
      "{'CrossEntropyLoss2d': 8162946.048, 'iou_mapillary': 0.06102318224310875, 'accuracy_mapillary': 0.32364508438110345}\n",
      "\n",
      "Epoch: 7\n",
      "train: 100%|██████████| 1125/1125 [12:23<00:00,  1.51it/s, CrossEntropyLoss2d - 7.142e+06, iou_mapillary - 0.1092, accuracy_mapillary - 0.8104]\n",
      "valid: 100%|██████████| 125/125 [00:23<00:00,  5.30it/s, CrossEntropyLoss2d - 8.127e+06, iou_mapillary - 0.05509, accuracy_mapillary - 0.34]  \n",
      "{'CrossEntropyLoss2d': 8127115.8319999995, 'iou_mapillary': 0.05509347680211066, 'accuracy_mapillary': 0.3399521255493166}\n"
     ]
    }
   ],
   "source": [
    "# train model for 8 epochs\n",
    "max_score = 0\n",
    "for i in range(8):\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(val_loader)\n",
    "    \n",
    "    print(valid_logs)\n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_mapillary']:\n",
    "        max_score = valid_logs['iou_mapillary']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 14 12:46:50 UTC 2020\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
